import json
import nltk
from nltk.corpus import wordnet as wn
from nltk.stem import WordNetLemmatizer
from tqdm import tqdm

# --- 1. ì„¤ì • ë° ë°ì´í„° ë‹¤ìš´ë¡œë“œ ---
# NLTK ë°ì´í„° ë‹¤ìš´ë¡œë“œ (ìµœì´ˆ 1íšŒ ì‹¤í–‰ í•„ìš”, ì´ë¯¸ ë˜ì–´ìˆë‹¤ë©´ ì£¼ì„ ì²˜ë¦¬ ê°€ëŠ¥)
print("NLTK ë°ì´í„° ë‹¤ìš´ë¡œë“œ ì¤‘...")
try:
    nltk.data.find('tokenizers/punkt_tab')
except LookupError:
    nltk.download('averaged_perceptron_tagger_eng')
    nltk.download('wordnet')
    nltk.download('omw-1.4')
    nltk.download('punkt_tab')
    nltk.download('averaged_perceptron_tagger') # ê¸°ë³¸ íƒœê±° ì¶”ê°€
print("ë‹¤ìš´ë¡œë“œ ì™„ë£Œ.")

# ë¶„ë¥˜ê¸° ì´ˆê¸°í™”
lemmatizer = WordNetLemmatizer()

# --- 2. í•µì‹¬ ë¡œì§ í•¨ìˆ˜ ---
def classify_word(word):
    """
    ë‹¨ì–´ê°€ ì¥ì†Œ(Place)ì¸ì§€ ê°ì²´(Object)ì¸ì§€ WordNetì„ í†µí•´ ë¶„ë¥˜
    """
    lemma = lemmatizer.lemmatize(word.lower())
    synsets = wn.synsets(lemma, pos=wn.NOUN)
    
    if not synsets:
        return "Unknown"

    synset = synsets[0] # ê°€ì¥ ì£¼ëœ ì˜ë¯¸ ì‚¬ìš©
    hypernym_paths = synset.hypernym_paths()
    
    for path in hypernym_paths:
        for hypernym in path:
            if hypernym.name() in ['location.n.01', 'place.n.01', 'geographical_area.n.01']:
                return "Place"
            if hypernym.name() in ['artifact.n.01', 'living_thing.n.01', 'structure.n.01']:
                return "Object"
                
    return "Unknown"

def extract_from_objects_json(file_path):
    print(f"\nProcessing {file_path}...")
    extracted_data = {'Place': set(), 'Object': set()}
    
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
    except FileNotFoundError:
        print(f"ì˜¤ë¥˜: {file_path} íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return extracted_data

    # [ìˆ˜ì •ë¨] ì „ì²´ ë°ì´í„°ë¥¼ ì²˜ë¦¬í•˜ê¸° ìœ„í•´ ìŠ¬ë¼ì´ì‹±([:100]) ì œê±°
    for entry in tqdm(data, desc="Objects JSON ì²˜ë¦¬ ì¤‘"): 
        for obj in entry.get('objects', []):
            names = obj.get('names', [])
            for name in names:
                category = classify_word(name)
                if category in ['Place', 'Object']:
                    extracted_data[category].add(name)
                    
    return extracted_data

def extract_from_regions_json(file_path):
    print(f"\nProcessing {file_path}...")
    extracted_data = {'Place': set(), 'Object': set()}
    
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
    except FileNotFoundError:
        print(f"ì˜¤ë¥˜: {file_path} íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return extracted_data
        
    # [ìˆ˜ì •ë¨] ì „ì²´ ë°ì´í„°ë¥¼ ì²˜ë¦¬í•˜ê¸° ìœ„í•´ ìŠ¬ë¼ì´ì‹±([:100]) ì œê±°
    for entry in tqdm(data, desc="Regions JSON ì²˜ë¦¬ ì¤‘"): 
        for region in entry.get('regions', []):
            phrase = region.get('phrase', "")
            
            # ë¬¸ì¥ì—ì„œ ëª…ì‚¬ë§Œ ì¶”ì¶œ
            tokens = nltk.word_tokenize(phrase)
            tags = nltk.pos_tag(tokens)
            
            for word, tag in tags:
                if tag.startswith('NN'): # ëª…ì‚¬
                    category = classify_word(word)
                    if category in ['Place', 'Object']:
                        extracted_data[category].add(word)
    
    return extracted_data

def save_list_to_txt(data_set, filename):
    """
    ì§‘í•©(Set) ë°ì´í„°ë¥¼ í…ìŠ¤íŠ¸ íŒŒì¼ë¡œ ì €ì¥í•˜ëŠ” í•¨ìˆ˜
    """
    print(f"\níŒŒì¼ ì €ì¥ ì¤‘: {filename} ...")
    sorted_list = sorted(list(data_set)) # ê°€ë‚˜ë‹¤ìˆœ ì •ë ¬
    
    with open(filename, 'w', encoding='utf-8') as f:
        for item in sorted_list:
            f.write(f"{item}\n")
    
    print(f"ì™„ë£Œ! ({len(sorted_list)}ê°œ ì €ì¥ë¨)")

# --- 3. ë©”ì¸ ì‹¤í–‰ ë¶€ë¶„ ---

# ì‹¤ì œ íŒŒì¼ ê²½ë¡œ (í™˜ê²½ì— ë§ê²Œ ìˆ˜ì • í•„ìš”)
obj_path = './data/objects.json'
reg_path = './data/region_descriptions.json'

# ì¶”ì¶œ ì‹¤í–‰
results_obj = extract_from_objects_json(obj_path)
results_reg = extract_from_regions_json(reg_path)

# ê²°ê³¼ í•©ì¹˜ê¸° (Setì˜ í•©ì§‘í•© ì—°ì‚°ìœ¼ë¡œ ì¤‘ë³µ ì œê±°)
final_places = results_obj['Place'].union(results_reg['Place'])
final_objects = results_obj['Object'].union(results_reg['Object'])

print("\n" + "="*30)
print("  ê²°ê³¼ ìš”ì•½  ")
print("="*30)
print(f"ğŸ  ì¶”ì¶œëœ ì¥ì†Œ (Place): {len(final_places)}ê°œ")
print(f"ğŸ“¦ ì¶”ì¶œëœ ê°ì²´ (Object): {len(final_objects)}ê°œ")

# íŒŒì¼ë¡œ ì €ì¥
save_list_to_txt(final_places, 'extracted_places.txt')
save_list_to_txt(final_objects, 'extracted_objects.txt')

print("\nëª¨ë“  ì‘ì—…ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")