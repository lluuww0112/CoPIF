# 26/02/12

생성일: 2026년 2월 12일 오후 2:22
태그: 데이터 생성, 아이디어 도출

# CoPIF : Coordinate Prompted Image Free

## 1. 선행 연구

---

> CapDec
> 
> - 사전학습된 CLIP을 그대로 사용
> - CLIP의 global vector를 이용해 captioning 모델을 image-free training을 진행
> - 이 때 image-text간의 실세계 gap을 좁히기 위해 가우시안 노이즈를 추가
> 
> IFseg
> 
> - 사전학습된 VLM의 정렬 능력을 전제
> - VLM의 임베딩 벡터를 이용해 feature map을 모사
> - segmentation VL model에 모사된 feature map을 입력, 최종 결과는 각 feature map 패치에 대응되는 logit을 이용해 분류
> - 예를 들어 고양이로 분류된 패치 지역은 고양이 segmentation 결과임
> 
> CLOSE
> 
> - image-text 간의 gap을 줄이기 위한 다양한 방법들을 실험
> - 실험 결과 text에 가우시안 노이즈를 추가하여 VLM에 입력하면 vision task 수행 능력이 향상됨을 입증
> 

## 2. 기존 Image Free의 문제점

---

> CapDec
> 
> - global vector만을 이용하여 downstream 모델을 학습
> - global vector는 맥락만을 담고 있을 뿐 위치 정보를 반영하기에는 어려움
> - 따라서 해당 논문에서도 captioning만을 수행함
> 
> IFseg
> 
> - segmentation에서는 객체의 위치가 중요할 뿐 이미지 상에 존재하는 객체가 어느 맥락에 위치하고 있는지는 중요하지 않음
> - 따라서 IFseg에서는 feature map을 만들 때 맥락이 담긴 vector를  사용하는 것이 아닌 임베딩 테이블의 벡터를 가져와 feature map모사
> - 예를 들어 흰 고양이와, 검은 고양이, 또는 흰 고양이 옆에 있는 검은 고양이 등등과 같은 정보를 feature map에 담아내지는 못함
> - 이로 인해 visual grounding 관련 task는 수행하지 못함
> 

## 3. CoPIF의 아이디어

---

> CLIP을 이용하면 맥락이 담긴 feature map을 모사할 수 있지 않을까?
> 
> - IFseg에서 가장 중요한 전제는 “사전학습된 VLM은 이미지와 텍스트 간의 정렬능력을 가지고 있다”임
> - 그렇다면 사전학습된 VLM말고 CLIP 또한 feature map 생성에 사용할 수 있는 것 아닌가?
> - 또 CLIP은 단순 단어 임베딩 뿐만 아니라 문장 임베딩 또한 가능
> - 결과적으로 IFseg에서 feature map을 구성했던 것과 비슷하게 CLIP으로 feature map을 구성한다면 REC와 같은 visul grounding또한 image free training이 가능해질 것이라 판단
> 
> 노이즈 주입
> 
> - 여러 연구에서 두 모달리티 간의 gap을 보간하기 위해 다양한 방법이 제안되어 왔지만 그 중 가장 효과적이라고 많이 언급되고 있는 것은 당연코 노이즈 주입임
> - 선행 연구 CLOSE에서도 단순 노이즈 주입, linear adapter, gap mean shift 등을 사용했지만 가장 범용적으로 높은 성능을 보이는 방법은 가우시안 노이즈 주입이며 노이즈 스케일을 하이퍼 파라미터로 취급, 각 task에 대해서 최적의 값을 제안하기도 했음
> 

## 4. Feature Map은 어떻게 만들어야 할까?

---

1. LLM으로 참조 표현과 정규화된 bbox를 생성 
2. 참조 표현과 bbox를 반영하여 랜덤 사이즈의 그리드에 참조 표현을 위치, 나머지 영역은 참조 표현에 등장하는 물체, 배경 텍스트를 위치시키고 없다면 맥락을 추론하여 채워 넣음
3. 만들어진 csv파일을 CLIP text encoder로 임베딩 하고 concat하여 최종 feature map을 생성
4. 랜덤 사이즈 그리드 맵을 이미지 입력에 맞게 변형
5. image free 수행

## 5. feature map 생성을 위한 프롬프팅

---

| 참조 표현 종류 | 설명 | 예시 |
| --- | --- | --- |
| 속성 기반 참조 | 객체 고유의 속성이 묘사됨 | **색상 및 패턴:** "파란색 줄무늬 셔츠", "점박이 강아지"
**크기 및 모양:** "동그란 탁자", "가장 작은 상자"
**재질:** "나무로 된 의자", "유리병"**상태:** "깨진 창문", "열려 있는 문" |
| 공간 관계 기반 참조 | 주변 객체와의 상대적 위치를 이용해 대상을 특정 | **상대적 위치:** "냉장고 **왼쪽**에 있는 컵", "소파 **뒤**의 스탠드"
**중첩/포함:** "바구니 **안**의 사과", "책상 **위**에 놓인 노트북"
**전역적 위치:** "오른쪽 상단의 구름", "가장 가운데 있는 사람" |
| 비교 기반 참조 | 동일한 클래스의 객체가 여러 개 있을 때, 상대적 차이를 강조 | **최하급 :** “가장 크기가 작은 고양이”,
**최상급:** "가장 키가 큰 해바라기", "가장 밝은 전등"
**비교급:** "다른 차들보다 더 깨끗한 트럭", "왼쪽 건물보다 낮은 담장" |
| 행위 및 기능 기반 참조 | 객체가 수행 중인 동작이나 상호작용을 묘사 | **동작:** "자전거를 타고 있는 남자", "잠을 자고 있는 고양이"
**상호작용:** "아이를 안고 있는 여성", "서로 마주 보고 있는 두 사람" |
| 부분-전체 관계 참조 | 객체의 특정 부위를 지칭하거나, 큰 객체의 일부분을 통해 대상을 특정 | **부위 지칭:** "자동차의 앞바퀴", "강아지의 꼬리"
**소속 기반:** "노란색 지붕을 가진 집", "검은색 넥타이를 맨 신사" |
| 조합형 참조 | 나머지 참조 표현들이 조합된 문장 |  |

```python
# REC 데이터셋 자가 생성(End-to-End)을 위한 시스템 프롬프트
description = """
[System]
너는 복잡한 시각적 장면을 시뮬레이션하고, 그 안에서 특정 객체를 지칭하는 [데이터셋 생성 엔진]이야. 
사용자가 별도의 객체 정보를 주지 않아도, 너 스스로 장면을 설계하고 그 안에서 '변별력 있는 참조 표현'을 생성해야 한다.

[User]
### [수행 단계]
1. [장면 설정]: 구체적인 장소와 상황을 설정해라. (예: 붐비는 지하철역, 식재료가 널브러진 주방 등)
2. [타겟 선정]: 해당 장소에서 다른 물체와 혼동될 수 있는 특정 객체를 타겟으로 정해라.
3. [참조 생성]: 아래 5가지 참조 유형을 골고루 사용하여 타겟을 유일하게 식별하는 문장을 생성해라.

### [참조 표현의 5가지 핵심 카테고리]
- Attribute: 색상, 재질, 크기, 상태 (예: "가장 작은", "깨진")
- Spatial: 주변 객체와의 상대적 위치 (예: "~ 뒤에 있는", "오른쪽 끝의")
- Comparative: 동일 클래스 내 차이 (예: "더 밝은 색의", "가장 키가 큰")
- Action: 현재 수행 중인 동작 (예: "전화 중인", "앉아 있는")
- Part-of-Whole: 객체의 세부 부위 (예: "바퀴에 진흙이 묻은", "손잡이가 금색인")

### [생성 원칙 - 기계적 패턴 방지]
- '자연스러운 발화': "객체는 ~에 있다"는 식의 딱딱한 설명을 금지한다. 사람이 사진 속 물건을 가리키며 말하듯 유연하게 작성하라.
- '다양한 문장 구조': 도치법, 의문문, 생략 등을 활용해 문장의 시작과 끝이 매번 다르게 하라.
- '변별력 강제': 반드시 주변에 비슷한 방해물(Distractors)이 있다고 가정하고, 그것들과 확실히 구분되는 정보를 문장에 포함하라.

### [출력 형식 (JSON List)]
아래 형식의 JSON 리스트로 응답하라:
[
  {
    "scene": "장면의 구체적인 묘사",
    "target_object": "지칭할 핵심 객체명",
    "referring_expressions": [
      "자연스러운 참조 표현 1",
      "자연스러운 참조 표현 2",
      "자연스러운 참조 표현 3"
    ],
    "category_mix": ["사용한 카테고리 기입 (예: Spatial, Action)"]
  }
]
"""

# 사용 예시:
# "주방 상황에서 5개의 데이터를 생성해줘" 또는 "자율주행 환경에서 10개의 데이터를 생성해줘"
# 라고만 명령하면 LLM이 알아서 객체를 잡고 문장을 만듭니다.
```

```python
# REC 공간 레이아웃 생성을 위한 최종 시스템 프롬프트
description = """
[User]
너는 텍스트로 된 참조 표현(Referring Expression)을 시각적 공간 좌표로 변환하는 [공간 지능 설계자]야.
사용자가 참조 표현과 그리드 사이즈를 제공하면, 너는 해당 문장이 묘사하는 장면을 상상하여 그리드 맵과 정규화된 BBox 데이터를 생성해야 한다.

### [수행 프로세스]
1. [의미론적 해석]: 입력된 문장에서 '타겟 객체', '참조 객체(Anchor)', 그리고 그들 간의 '공간적 관계'를 분석하라.
2. [배경 풍성함(Clutter) 생성]: 타겟과 관련 없지만 장면에 어울리는 5~8개의 다양한 방해 객체(Distractors)를 생성하여 배치하라. 
   - 절대 "배경"이나 "빈 공간"으로만 채우지 말고, 구체적인 사물 이름(예: '나무 벤치', '길고양이', '버려진 캔')을 사용하라.
3. [그리드 맵 설계]: 지정된 {grid_size} x {grid_size} 행렬에 객체들을 배치하라.
   - 모든 객체는 단순 사각형이 아닌, 인접한 셀들을 뭉쳐서(Clumped) 비정형적인 형태로 표현하라.
4. [좌표 정규화]: 0.0 ~ 1.0 사이의 float 값으로 [xmin, ymin, xmax, ymax] BBox를 계산하라. 
   - 좌표 계산식: (인덱스 / grid_size)

### [객체 배치 원칙]
- [논리적 일치]: "A의 왼쪽"이라는 입력이 들어오면, 그리드 상에서 A의 x좌표보다 타겟의 x좌표가 반드시 작아야 한다.
- [밀도 최적화]: 그리드의 60% 이상이 다양한 객체(타겟 + 방해 객체)로 채워지도록 설계하라.
- [참조 표현 스타일]: 
  - 타겟: 입력받은 문장을 그대로 사용.
  - 배경 객체: "짧고 명확한 참조 표현" (예: "낡은 나무 의자")

### [출력 형식 (JSON)]
{
  "input_query": "입력받은 참조 표현",
  "grid_dimension": "N * N",
  "grid_map": [[...], [...]],
  "annotations": [
    {
      "label": "전체 문장 또는 객체명",
      "type": "target | anchor | background",
      "bbox": [xmin, ymin, xmax, ymax]
    }
  ]
}
"""

# 유저 입력 템플릿 예시:
# user_input = {
#     "query": "흐린 하늘에서 전투기 위쪽에서 날고 있는 여객기",
#     "grid_size": 16
# }
```

```python
# 그리드 예시
{
  "input_query": "파란 차 왼쪽에 서 있는 흰 셔츠를 입은 여자",
  "grid_dimension": "10 x 10",
  "grid_map": [
    ["낡은 가로등","낡은 가로등","흰 셔츠를 입은 여자","흰 셔츠를 입은 여자","작은 화단","작은 화단","파란 차","파란 차","파란 차","가로수"],
    ["낡은 가로등","흰 셔츠를 입은 여자","흰 셔츠를 입은 여자","흰 셔츠를 입은 여자","작은 화단","작은 화단","파란 차","파란 차","파란 차","가로수"],
    ["길고양이","길고양이","흰 셔츠를 입은 여자","흰 셔츠를 입은 여자","작은 화단","버려진 캔","파란 차","파란 차","파란 차","가로수"],
    ["길고양이","길고양이","낡은 나무 벤치","낡은 나무 벤치","낡은 나무 벤치","버려진 캔","버려진 캔","파란 차","파란 차","가로수"],
    ["작은 화단","작은 화단","낡은 나무 벤치","낡은 나무 벤치","낡은 나무 벤치","신문 가판대","신문 가판대","신문 가판대","가로수","가로수"],
    ["작은 화단","작은 화단","자전거","자전거","자전거","신문 가판대","신문 가판대","신문 가판대","가로수","가로수"],
    ["자전거","자전거","자전거","자전거","자전거","자전거","자전거","자전거","가로수","가로수"],
    ["택배 상자 더미","택배 상자 더미","택배 상자 더미","택배 상자 더미","자전거","자전거","자전거","자전거","가로수","가로수"],
    ["택배 상자 더미","택배 상자 더미","택배 상자 더미","택배 상자 더미","자전거","자전거","자전거","자전거","가로수","가로수"],
    ["택배 상자 더미","택배 상자 더미","택배 상자 더미","택배 상자 더미","자전거","자전거","자전거","자전거","가로수","가로수"]
  ],
  "annotations": [
    {
      "label": "파란 차 왼쪽에 서 있는 흰 셔츠를 입은 여자",
      "type": "target",
      "bbox": [0.1, 0.0, 0.4, 0.3]
    },
    {
      "label": "파란 차",
      "type": "anchor",
      "bbox": [0.6, 0.0, 0.9, 0.4]
    },
    {
      "label": "낡은 가로등",
      "type": "background",
      "bbox": [0.0, 0.0, 0.2, 0.2]
    },
    {
      "label": "길고양이",
      "type": "background",
      "bbox": [0.0, 0.2, 0.2, 0.4]
    },
    {
      "label": "작은 화단",
      "type": "background",
      "bbox": [0.0, 0.0, 0.6, 0.6]
    },
    {
      "label": "낡은 나무 벤치",
      "type": "background",
      "bbox": [0.2, 0.3, 0.5, 0.5]
    },
    {
      "label": "버려진 캔",
      "type": "background",
      "bbox": [0.5, 0.2, 0.7, 0.4]
    },
    {
      "label": "신문 가판대",
      "type": "background",
      "bbox": [0.5, 0.4, 0.8, 0.6]
    },
    {
      "label": "자전거",
      "type": "background",
      "bbox": [0.0, 0.5, 0.8, 1.0]
    },
    {
      "label": "택배 상자 더미",
      "type": "background",
      "bbox": [0.0, 0.7, 0.4, 1.0]
    },
    {
      "label": "가로수",
      "type": "background",
      "bbox": [0.8, 0.0, 1.0, 1.0]
    }
  ]
}

```
